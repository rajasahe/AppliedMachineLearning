{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542e8b1d",
   "metadata": {},
   "source": [
    "# SMS Spam Classification: Model Training & Evaluation\n",
    "\n",
    "This notebook covers model training, scoring, evaluation, validation, hyperparameter tuning, and benchmarking for SMS spam classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa9363",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "We begin by importing all necessary libraries for feature extraction, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b736d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df303d8",
   "metadata": {},
   "source": [
    "## 2. Load Data Splits\n",
    "Load the train, validation, and test CSV files for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed9df6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4457, 3)\n",
      "Validation shape: (557, 3)\n",
      "Test shape: (558, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load train, validation, and test splits\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "val_df = pd.read_csv('data/validation.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Validation shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896aa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no NaN values in 'text' columns before feature extraction\n",
    "train_df = train_df.dropna(subset=['text'])\n",
    "val_df = val_df.dropna(subset=['text'])\n",
    "test_df = test_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9f76b",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and Preprocessing\n",
    "Convert SMS text into numerical features using TF-IDF or CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fff3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature matrix shape (train): (4456, 8356)\n"
     ]
    }
   ],
   "source": [
    "# Use TF-IDF for feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_val = vectorizer.transform(val_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_train = train_df['label_num']\n",
    "y_val = val_df['label_num']\n",
    "y_test = test_df['label_num']\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape (train): {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d12d29",
   "metadata": {},
   "source": [
    "## 4. Define and Fit Models on Training Data\n",
    "We will define and train three benchmark models: Logistic Regression, Multinomial Naive Bayes, and Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73036935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression trained.\n",
      "MultinomialNB trained.\n",
      "LinearSVC trained.\n"
     ]
    }
   ],
   "source": [
    "# Define and fit models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LinearSVC': LinearSVC(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb037ad",
   "metadata": {},
   "source": [
    "## 5. Score Models on Given Data\n",
    "We will write functions to score the trained models on train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b2cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "LogisticRegression on Train - Accuracy: 0.9711, F1: 0.8796\n",
      "LogisticRegression on Validation - Accuracy: 0.9623, F1: 0.8346\n",
      "\n",
      "MultinomialNB:\n",
      "MultinomialNB on Train - Accuracy: 0.9681, F1: 0.8653\n",
      "MultinomialNB on Validation - Accuracy: 0.9515, F1: 0.7769\n",
      "\n",
      "LinearSVC:\n",
      "LinearSVC on Train - Accuracy: 0.9993, F1: 0.9975\n",
      "LinearSVC on Validation - Accuracy: 0.9820, F1: 0.9286\n"
     ]
    }
   ],
   "source": [
    "# Function to score models\n",
    "def score_model(model, X, y, dataset_name):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    print(f\"{model.__class__.__name__} on {dataset_name} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    return acc, f1\n",
    "\n",
    "# Score all models on train and validation sets\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    score_model(model, X_train, y_train, 'Train')\n",
    "    score_model(model, X_val, y_val, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fc402",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Predictions\n",
    "We will evaluate model predictions using confusion matrix, precision, recall, F1-score, and provide clear explanations of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe9f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for LogisticRegression on Validation:\n",
      "[[483   0]\n",
      " [ 21  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       483\n",
      "           1       1.00      0.72      0.83        74\n",
      "\n",
      "    accuracy                           0.96       557\n",
      "   macro avg       0.98      0.86      0.91       557\n",
      "weighted avg       0.96      0.96      0.96       557\n",
      "\n",
      "\n",
      "Evaluation for MultinomialNB on Validation:\n",
      "[[483   0]\n",
      " [ 27  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       483\n",
      "           1       1.00      0.64      0.78        74\n",
      "\n",
      "    accuracy                           0.95       557\n",
      "   macro avg       0.97      0.82      0.87       557\n",
      "weighted avg       0.95      0.95      0.95       557\n",
      "\n",
      "\n",
      "Evaluation for LinearSVC on Validation:\n",
      "[[482   1]\n",
      " [  9  65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       483\n",
      "           1       0.98      0.88      0.93        74\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.98      0.94      0.96       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate model predictions\n",
    "def evaluate_model(model, X, y, dataset_name):\n",
    "    y_pred = model.predict(X)\n",
    "    print(f\"\\nEvaluation for {model.__class__.__name__} on {dataset_name}:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "# Evaluate all models on validation set\n",
    "for name, model in models.items():\n",
    "    evaluate_model(model, X_val, y_val, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fd77b",
   "metadata": {},
   "source": [
    "## 7. Validation: Fit, Score, and Evaluate on Train and Validation Sets\n",
    "We will validate the models by fitting, scoring, and evaluating on both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6946b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression - Train Set:\n",
      "LogisticRegression on Train - Accuracy: 0.9711, F1: 0.8796\n",
      "\n",
      "Evaluation for LogisticRegression on Train:\n",
      "[[3856    2]\n",
      " [ 127  471]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3858\n",
      "           1       1.00      0.79      0.88       598\n",
      "\n",
      "    accuracy                           0.97      4456\n",
      "   macro avg       0.98      0.89      0.93      4456\n",
      "weighted avg       0.97      0.97      0.97      4456\n",
      "\n",
      "\n",
      "Logistic Regression - Validation Set:\n",
      "LogisticRegression on Validation - Accuracy: 0.9623, F1: 0.8346\n",
      "\n",
      "Evaluation for LogisticRegression on Validation:\n",
      "[[483   0]\n",
      " [ 21  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       483\n",
      "           1       1.00      0.72      0.83        74\n",
      "\n",
      "    accuracy                           0.96       557\n",
      "   macro avg       0.98      0.86      0.91       557\n",
      "weighted avg       0.96      0.96      0.96       557\n",
      "\n",
      "\n",
      "MultinomialNB - Train Set:\n",
      "MultinomialNB on Train - Accuracy: 0.9681, F1: 0.8653\n",
      "\n",
      "Evaluation for MultinomialNB on Train:\n",
      "[[3858    0]\n",
      " [ 142  456]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      3858\n",
      "           1       1.00      0.76      0.87       598\n",
      "\n",
      "    accuracy                           0.97      4456\n",
      "   macro avg       0.98      0.88      0.92      4456\n",
      "weighted avg       0.97      0.97      0.97      4456\n",
      "\n",
      "\n",
      "MultinomialNB - Validation Set:\n",
      "MultinomialNB on Validation - Accuracy: 0.9515, F1: 0.7769\n",
      "\n",
      "Evaluation for MultinomialNB on Validation:\n",
      "[[483   0]\n",
      " [ 27  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       483\n",
      "           1       1.00      0.64      0.78        74\n",
      "\n",
      "    accuracy                           0.95       557\n",
      "   macro avg       0.97      0.82      0.87       557\n",
      "weighted avg       0.95      0.95      0.95       557\n",
      "\n",
      "\n",
      "LinearSVC - Train Set:\n",
      "LinearSVC on Train - Accuracy: 0.9993, F1: 0.9975\n",
      "\n",
      "Evaluation for LinearSVC on Train:\n",
      "[[3858    0]\n",
      " [   3  595]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3858\n",
      "           1       1.00      0.99      1.00       598\n",
      "\n",
      "    accuracy                           1.00      4456\n",
      "   macro avg       1.00      1.00      1.00      4456\n",
      "weighted avg       1.00      1.00      1.00      4456\n",
      "\n",
      "\n",
      "LinearSVC - Validation Set:\n",
      "LinearSVC on Validation - Accuracy: 0.9820, F1: 0.9286\n",
      "\n",
      "Evaluation for LinearSVC on Validation:\n",
      "[[482   1]\n",
      " [  9  65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       483\n",
      "           1       0.98      0.88      0.93        74\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.98      0.94      0.96       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation: Fit, Score, and Evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} - Train Set:\")\n",
    "    score_model(model, X_train, y_train, 'Train')\n",
    "    evaluate_model(model, X_train, y_train, 'Train')\n",
    "    print(f\"\\n{name} - Validation Set:\")\n",
    "    score_model(model, X_val, y_val, 'Validation')\n",
    "    evaluate_model(model, X_val, y_val, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a5141",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "We will fine-tune model hyperparameters using GridSearchCV on train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c697893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10}\n",
      "Best F1 (train): 0.8772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       483\n",
      "           1       1.00      0.88      0.94        74\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.99      0.94      0.96       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Hyperparameter tuning for Logistic Regression\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "gs = GridSearchCV(LogisticRegression(max_iter=1000, random_state=42), param_grid, cv=3, scoring='f1')\n",
    "gs.fit(X_train, y_train)\n",
    "print(f\"Best params: {gs.best_params_}\")\n",
    "print(f\"Best F1 (train): {gs.best_score_:.4f}\")\n",
    "# Evaluate on validation set\n",
    "y_val_pred = gs.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c9609",
   "metadata": {},
   "source": [
    "## 9. Benchmark and Select Best Model on Test Data\n",
    "We will score all benchmark models on the test set, compare their performance, and select the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a0e6e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression - Test Set:\n",
      "LogisticRegression on Test - Accuracy: 0.9695, F1: 0.8722\n",
      "\n",
      "Evaluation for LogisticRegression on Test:\n",
      "[[482   0]\n",
      " [ 17  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       482\n",
      "           1       1.00      0.77      0.87        75\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.98      0.89      0.93       557\n",
      "weighted avg       0.97      0.97      0.97       557\n",
      "\n",
      "\n",
      "MultinomialNB - Test Set:\n",
      "MultinomialNB on Test - Accuracy: 0.9533, F1: 0.7903\n",
      "\n",
      "Evaluation for MultinomialNB on Test:\n",
      "[[482   0]\n",
      " [ 26  49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       482\n",
      "           1       1.00      0.65      0.79        75\n",
      "\n",
      "    accuracy                           0.95       557\n",
      "   macro avg       0.97      0.83      0.88       557\n",
      "weighted avg       0.96      0.95      0.95       557\n",
      "\n",
      "\n",
      "LinearSVC - Test Set:\n",
      "LinearSVC on Test - Accuracy: 0.9856, F1: 0.9452\n",
      "\n",
      "Evaluation for LinearSVC on Test:\n",
      "[[480   2]\n",
      " [  6  69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       482\n",
      "           1       0.97      0.92      0.95        75\n",
      "\n",
      "    accuracy                           0.99       557\n",
      "   macro avg       0.98      0.96      0.97       557\n",
      "weighted avg       0.99      0.99      0.99       557\n",
      "\n",
      "\n",
      "Tuned Logistic Regression - Test Set:\n",
      "Accuracy: 0.9803, F1: 0.9220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       482\n",
      "           1       0.98      0.87      0.92        75\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.98      0.93      0.96       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "\n",
      "Best model on test set: LinearSVC (F1: 0.9452)\n"
     ]
    }
   ],
   "source": [
    "# Score all models on test set and select the best\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} - Test Set:\")\n",
    "    acc, f1 = score_model(model, X_test, y_test, 'Test')\n",
    "    results[name] = {'accuracy': acc, 'f1': f1}\n",
    "    evaluate_model(model, X_test, y_test, 'Test')\n",
    "\n",
    "# Add tuned Logistic Regression if available\n",
    "if 'gs' in locals():\n",
    "    print(\"\\nTuned Logistic Regression - Test Set:\")\n",
    "    acc = accuracy_score(y_test, gs.predict(X_test))\n",
    "    f1 = f1_score(y_test, gs.predict(X_test))\n",
    "    print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    print(classification_report(y_test, gs.predict(X_test)))\n",
    "    results['Tuned Logistic Regression'] = {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# Select best model\n",
    "best_model = max(results, key=lambda k: results[k]['f1'])\n",
    "print(f\"\\nBest model on test set: {best_model} (F1: {results[best_model]['f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cc2f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Model Training & Evaluation Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
