{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# SMS Spam Classification with MLflow\n",
                "\n",
                "This notebook demonstrates experiment tracking and model version control using MLflow.\n",
                "We'll build 3 benchmark models and track their performance using AUCPR (Area Under Precision-Recall Curve) as the primary metric."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports-header",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries imported successfully!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    roc_auc_score,\n",
                "    average_precision_score,\n",
                "    precision_recall_curve,\n",
                "    auc\n",
                ")\n",
                "from sklearn.pipeline import Pipeline\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mlflow-setup-header",
            "metadata": {},
            "source": [
                "## 2. MLflow Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "mlflow-setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
                        "2026/02/16 02:38:15 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
                        "2026/02/16 02:38:18 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
                        "2026/02/16 02:38:18 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Experiment Name: SMS_Spam_Classification\n",
                        "Experiment ID: 1\n",
                        "Artifact Location: file:///C:/Users/Raja/AML/AML_DVC/mlruns/1\n",
                        "\n",
                        "MLflow Tracking URI: sqlite:///mlflow.db\n"
                    ]
                }
            ],
            "source": [
                "# Set experiment name\n",
                "EXPERIMENT_NAME = \"SMS_Spam_Classification\"\n",
                "mlflow.set_experiment(EXPERIMENT_NAME)\n",
                "\n",
                "# Get experiment info\n",
                "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "print(f\"Experiment Name: {experiment.name}\")\n",
                "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
                "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
                "print(f\"\\nMLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-load-header",
            "metadata": {},
            "source": [
                "## 3. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "data-load",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training samples: 4457\n",
                        "Validation samples: 557\n",
                        "Test samples: 558\n",
                        "\n",
                        "Class distribution in training set:\n",
                        "label\n",
                        "ham     3857\n",
                        "spam     600\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Spam ratio: 13.46%\n"
                    ]
                }
            ],
            "source": [
                "# Load datasets\n",
                "train_df = pd.read_csv('train.csv')\n",
                "val_df = pd.read_csv('validation.csv')\n",
                "test_df = pd.read_csv('test.csv')\n",
                "\n",
                "print(f\"Training samples: {len(train_df)}\")\n",
                "print(f\"Validation samples: {len(val_df)}\")\n",
                "print(f\"Test samples: {len(test_df)}\")\n",
                "print(f\"\\nClass distribution in training set:\")\n",
                "print(train_df['label'].value_counts())\n",
                "print(f\"\\nSpam ratio: {(train_df['label'] == 'spam').sum() / len(train_df):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-prep-header",
            "metadata": {},
            "source": [
                "## 4. Prepare Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "data-prep",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data prepared successfully!\n",
                        "Training labels - Ham: 3857, Spam: 600\n",
                        "Validation labels - Ham: 490, Spam: 67\n",
                        "Test labels - Ham: 478, Spam: 80\n"
                    ]
                }
            ],
            "source": [
                "# Extract features and labels\n",
                "X_train = train_df['text'].values\n",
                "y_train = (train_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "X_val = val_df['text'].values\n",
                "y_val = (val_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "X_test = test_df['text'].values\n",
                "y_test = (test_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "print(\"Data prepared successfully!\")\n",
                "print(f\"Training labels - Ham: {(y_train == 0).sum()}, Spam: {(y_train == 1).sum()}\")\n",
                "print(f\"Validation labels - Ham: {(y_val == 0).sum()}, Spam: {(y_val == 1).sum()}\")\n",
                "print(f\"Test labels - Ham: {(y_test == 0).sum()}, Spam: {(y_test == 1).sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helper-header",
            "metadata": {},
            "source": [
                "## 5. Helper Functions for Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Helper functions defined!\n"
                    ]
                }
            ],
            "source": [
                "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
                "    \"\"\"Calculate comprehensive metrics including AUCPR\"\"\"\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred),\n",
                "        'recall': recall_score(y_true, y_pred),\n",
                "        'f1_score': f1_score(y_true, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
                "        'aucpr': average_precision_score(y_true, y_pred_proba)  # Area Under PR Curve\n",
                "    }\n",
                "    return metrics\n",
                "\n",
                "def print_metrics(metrics, dataset_name=\"\"):\n",
                "    \"\"\"Pretty print metrics\"\"\"\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"{dataset_name} Metrics\")\n",
                "    print(f\"{'='*50}\")\n",
                "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
                "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
                "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
                "    print(f\"F1 Score:  {metrics['f1_score']:.4f}\")\n",
                "    print(f\"ROC AUC:   {metrics['roc_auc']:.4f}\")\n",
                "    print(f\"AUCPR:     {metrics['aucpr']:.4f}\")\n",
                "    print(f\"{'='*50}\")\n",
                "\n",
                "print(\"Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model1-header",
            "metadata": {},
            "source": [
                "## 6. Model 1: Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "model1-train",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MLflow Run ID: ef234570f6fb4d01b833cb27c7aa0a06\n",
                        "\n",
                        "Training Logistic Regression...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026/02/16 02:38:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "Training Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9809\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.8583\n",
                        "F1 Score:  0.9238\n",
                        "ROC AUC:   0.9982\n",
                        "AUCPR:     0.9923\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Validation Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9785\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.8209\n",
                        "F1 Score:  0.9016\n",
                        "ROC AUC:   0.9980\n",
                        "AUCPR:     0.9887\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Test Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9624\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.7375\n",
                        "F1 Score:  0.8489\n",
                        "ROC AUC:   0.9852\n",
                        "AUCPR:     0.9766\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Registered model 'SMS_Spam_LogisticRegression' already exists. Creating a new version of this model...\n",
                        "Created version '2' of model 'SMS_Spam_LogisticRegression'.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úì Model registered as 'SMS_Spam_LogisticRegression'\n",
                        "‚úì Test AUCPR: 0.9766\n"
                    ]
                }
            ],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Logistic_Regression\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Logistic Regression',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'C': 1.0,\n",
                "        'max_iter': 1000,\n",
                "        'solver': 'liblinear'\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', LogisticRegression(C=1.0, max_iter=1000, solver='liblinear', random_state=42))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Logistic Regression...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_LogisticRegression\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\n‚úì Model registered as 'SMS_Spam_LogisticRegression'\")\n",
                "    print(f\"‚úì Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    lr_run_id = run.info.run_id\n",
                "    lr_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model2-header",
            "metadata": {},
            "source": [
                "## 7. Model 2: Multinomial Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "model2-train",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MLflow Run ID: b6b588f3dad340c3b088188158f0dd45\n",
                        "\n",
                        "Training Multinomial Naive Bayes...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026/02/16 02:39:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "Training Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9818\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.8650\n",
                        "F1 Score:  0.9276\n",
                        "ROC AUC:   0.9933\n",
                        "AUCPR:     0.9825\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Validation Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9820\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.8507\n",
                        "F1 Score:  0.9194\n",
                        "ROC AUC:   0.9938\n",
                        "AUCPR:     0.9784\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Test Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9677\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.7750\n",
                        "F1 Score:  0.8732\n",
                        "ROC AUC:   0.9840\n",
                        "AUCPR:     0.9675\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Registered model 'SMS_Spam_NaiveBayes' already exists. Creating a new version of this model...\n",
                        "Created version '2' of model 'SMS_Spam_NaiveBayes'.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úì Model registered as 'SMS_Spam_NaiveBayes'\n",
                        "‚úì Test AUCPR: 0.9675\n"
                    ]
                }
            ],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Naive_Bayes\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Multinomial Naive Bayes',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'alpha': 1.0\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', MultinomialNB(alpha=1.0))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Multinomial Naive Bayes...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_NaiveBayes\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\n‚úì Model registered as 'SMS_Spam_NaiveBayes'\")\n",
                "    print(f\"‚úì Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    nb_run_id = run.info.run_id\n",
                "    nb_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model3-header",
            "metadata": {},
            "source": [
                "## 8. Model 3: Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "model3-train",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MLflow Run ID: d4e5467716544f4d848533d2ee1563ca\n",
                        "\n",
                        "Training Random Forest...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026/02/16 02:39:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "Training Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9659\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.7467\n",
                        "F1 Score:  0.8550\n",
                        "ROC AUC:   0.9938\n",
                        "AUCPR:     0.9832\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Validation Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9695\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.7463\n",
                        "F1 Score:  0.8547\n",
                        "ROC AUC:   0.9943\n",
                        "AUCPR:     0.9765\n",
                        "==================================================\n",
                        "\n",
                        "==================================================\n",
                        "Test Metrics\n",
                        "==================================================\n",
                        "Accuracy:  0.9444\n",
                        "Precision: 1.0000\n",
                        "Recall:    0.6125\n",
                        "F1 Score:  0.7597\n",
                        "ROC AUC:   0.9841\n",
                        "AUCPR:     0.9636\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Registered model 'SMS_Spam_RandomForest' already exists. Creating a new version of this model...\n",
                        "Created version '2' of model 'SMS_Spam_RandomForest'.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úì Model registered as 'SMS_Spam_RandomForest'\n",
                        "‚úì Test AUCPR: 0.9636\n"
                    ]
                }
            ],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Random_Forest\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Random Forest',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'n_estimators': 100,\n",
                "        'max_depth': 20,\n",
                "        'min_samples_split': 5,\n",
                "        'min_samples_leaf': 2\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', RandomForestClassifier(\n",
                "            n_estimators=100,\n",
                "            max_depth=20,\n",
                "            min_samples_split=5,\n",
                "            min_samples_leaf=2,\n",
                "            random_state=42,\n",
                "            n_jobs=-1\n",
                "        ))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Random Forest...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_RandomForest\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\n‚úì Model registered as 'SMS_Spam_RandomForest'\")\n",
                "    print(f\"‚úì Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    rf_run_id = run.info.run_id\n",
                "    rf_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-header",
            "metadata": {},
            "source": [
                "## 9. Model Comparison - AUCPR Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "comparison",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "MODEL COMPARISON - AUCPR (Area Under Precision-Recall Curve)\n",
                        "======================================================================\n",
                        "\n",
                        "Model                          Test AUCPR      Val AUCPR       Train AUCPR    \n",
                        "----------------------------------------------------------------------\n",
                        "Logistic_Regression            0.9766          0.9887          0.9923         \n",
                        "Logistic_Regression            0.9766          0.9887          0.9923         \n",
                        "Naive_Bayes                    0.9675          0.9784          0.9825         \n",
                        "Naive_Bayes                    0.9675          0.9784          0.9825         \n",
                        "Random_Forest                  0.9636          0.9765          0.9832         \n",
                        "Random_Forest                  0.9636          0.9765          0.9832         \n",
                        "\n",
                        "======================================================================\n",
                        "\n",
                        "üèÜ BEST MODEL: Logistic_Regression\n",
                        "   Test AUCPR: 0.9766\n",
                        "   Run ID: 0313136c4a0b42db97fe3c3120198423\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Retrieve all runs from the experiment\n",
                "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
                "\n",
                "# Filter and display AUCPR metrics\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL COMPARISON - AUCPR (Area Under Precision-Recall Curve)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Sort by test AUCPR (descending)\n",
                "runs_sorted = runs.sort_values('metrics.test_aucpr', ascending=False)\n",
                "\n",
                "print(f\"\\n{'Model':<30} {'Test AUCPR':<15} {'Val AUCPR':<15} {'Train AUCPR':<15}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "for idx, row in runs_sorted.iterrows():\n",
                "    model_name = row['tags.mlflow.runName']\n",
                "    test_aucpr = row['metrics.test_aucpr']\n",
                "    val_aucpr = row['metrics.val_aucpr']\n",
                "    train_aucpr = row['metrics.train_aucpr']\n",
                "    \n",
                "    print(f\"{model_name:<30} {test_aucpr:<15.4f} {val_aucpr:<15.4f} {train_aucpr:<15.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "\n",
                "# Find best model\n",
                "best_model_row = runs_sorted.iloc[0]\n",
                "best_model_name = best_model_row['tags.mlflow.runName']\n",
                "best_aucpr = best_model_row['metrics.test_aucpr']\n",
                "\n",
                "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
                "print(f\"   Test AUCPR: {best_aucpr:.4f}\")\n",
                "print(f\"   Run ID: {best_model_row['run_id']}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retrieve-header",
            "metadata": {},
            "source": [
                "## 10. Retrieve and Print Individual Model AUCPR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "retrieve-models",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "INDIVIDUAL MODEL AUCPR RETRIEVAL FROM MLFLOW\n",
                        "======================================================================\n",
                        "\n",
                        "üìä Logistic_Regression\n",
                        "   Run ID: ef234570f6fb4d01b833cb27c7aa0a06\n",
                        "   Test AUCPR:       0.9766\n",
                        "   Validation AUCPR: 0.9887\n",
                        "   Training AUCPR:   0.9923\n",
                        "   --------------------------------------------------\n",
                        "\n",
                        "üìä Naive_Bayes\n",
                        "   Run ID: b6b588f3dad340c3b088188158f0dd45\n",
                        "   Test AUCPR:       0.9675\n",
                        "   Validation AUCPR: 0.9784\n",
                        "   Training AUCPR:   0.9825\n",
                        "   --------------------------------------------------\n",
                        "\n",
                        "üìä Random_Forest\n",
                        "   Run ID: d4e5467716544f4d848533d2ee1563ca\n",
                        "   Test AUCPR:       0.9636\n",
                        "   Validation AUCPR: 0.9765\n",
                        "   Training AUCPR:   0.9832\n",
                        "   --------------------------------------------------\n",
                        "\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Retrieve AUCPR for each model from MLflow\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"INDIVIDUAL MODEL AUCPR RETRIEVAL FROM MLFLOW\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "model_names = [\n",
                "    \"Logistic_Regression\",\n",
                "    \"Naive_Bayes\",\n",
                "    \"Random_Forest\"\n",
                "]\n",
                "\n",
                "for model_name in model_names:\n",
                "    # Search for runs with this name\n",
                "    runs = mlflow.search_runs(\n",
                "        experiment_ids=[experiment.experiment_id],\n",
                "        filter_string=f\"tags.mlflow.runName = '{model_name}'\",\n",
                "        order_by=[\"start_time DESC\"],\n",
                "        max_results=1\n",
                "    )\n",
                "    \n",
                "    if len(runs) > 0:\n",
                "        run = runs.iloc[0]\n",
                "        run_id = run['run_id']\n",
                "        test_aucpr = run['metrics.test_aucpr']\n",
                "        val_aucpr = run['metrics.val_aucpr']\n",
                "        train_aucpr = run['metrics.train_aucpr']\n",
                "        \n",
                "        print(f\"\\nüìä {model_name}\")\n",
                "        print(f\"   Run ID: {run_id}\")\n",
                "        print(f\"   Test AUCPR:       {test_aucpr:.4f}\")\n",
                "        print(f\"   Validation AUCPR: {val_aucpr:.4f}\")\n",
                "        print(f\"   Training AUCPR:   {train_aucpr:.4f}\")\n",
                "        print(f\"   {'-'*50}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "registry-header",
            "metadata": {},
            "source": [
                "## 11. View Registered Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "registry-view",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "REGISTERED MODELS IN MLFLOW MODEL REGISTRY\n",
                        "======================================================================\n",
                        "\n",
                        "Model Name: SMS_Spam_LogisticRegression\n",
                        "Description: N/A\n",
                        "Latest Version: 2\n",
                        "Last Updated: 1771189735767\n",
                        "----------------------------------------------------------------------\n",
                        "\n",
                        "Model Name: SMS_Spam_NaiveBayes\n",
                        "Description: N/A\n",
                        "Latest Version: 2\n",
                        "Last Updated: 1771189758129\n",
                        "----------------------------------------------------------------------\n",
                        "\n",
                        "Model Name: SMS_Spam_RandomForest\n",
                        "Description: N/A\n",
                        "Latest Version: 2\n",
                        "Last Updated: 1771189784981\n",
                        "----------------------------------------------------------------------\n",
                        "\n",
                        "‚úì All models successfully registered and tracked with MLflow!\n"
                    ]
                }
            ],
            "source": [
                "from mlflow.tracking import MlflowClient\n",
                "\n",
                "client = MlflowClient()\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"REGISTERED MODELS IN MLFLOW MODEL REGISTRY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# List all registered models\n",
                "registered_models = client.search_registered_models()\n",
                "\n",
                "for rm in registered_models:\n",
                "    if rm.name.startswith(\"SMS_Spam_\"):\n",
                "        print(f\"\\nModel Name: {rm.name}\")\n",
                "        print(f\"Description: {rm.description if rm.description else 'N/A'}\")\n",
                "        print(f\"Latest Version: {rm.latest_versions[0].version if rm.latest_versions else 'N/A'}\")\n",
                "        print(f\"Last Updated: {rm.last_updated_timestamp}\")\n",
                "        print(\"-\"*70)\n",
                "\n",
                "print(\"\\n‚úì All models successfully registered and tracked with MLflow!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
