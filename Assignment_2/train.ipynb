{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# SMS Spam Classification with MLflow\n",
                "\n",
                "This notebook demonstrates experiment tracking and model version control using MLflow.\n",
                "We'll build 3 benchmark models and track their performance using AUCPR (Area Under Precision-Recall Curve) as the primary metric."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports-header",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    roc_auc_score,\n",
                "    average_precision_score,\n",
                "    precision_recall_curve,\n",
                "    auc\n",
                ")\n",
                "from sklearn.pipeline import Pipeline\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mlflow-setup-header",
            "metadata": {},
            "source": [
                "## 2. MLflow Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mlflow-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set experiment name\n",
                "EXPERIMENT_NAME = \"SMS_Spam_Classification\"\n",
                "mlflow.set_experiment(EXPERIMENT_NAME)\n",
                "\n",
                "# Get experiment info\n",
                "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "print(f\"Experiment Name: {experiment.name}\")\n",
                "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
                "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
                "print(f\"\\nMLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-load-header",
            "metadata": {},
            "source": [
                "## 3. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-load",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load datasets\n",
                "train_df = pd.read_csv('train.csv')\n",
                "val_df = pd.read_csv('validation.csv')\n",
                "test_df = pd.read_csv('test.csv')\n",
                "\n",
                "print(f\"Training samples: {len(train_df)}\")\n",
                "print(f\"Validation samples: {len(val_df)}\")\n",
                "print(f\"Test samples: {len(test_df)}\")\n",
                "print(f\"\\nClass distribution in training set:\")\n",
                "print(train_df['label'].value_counts())\n",
                "print(f\"\\nSpam ratio: {(train_df['label'] == 'spam').sum() / len(train_df):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-prep-header",
            "metadata": {},
            "source": [
                "## 4. Prepare Data for Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-prep",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features and labels\n",
                "X_train = train_df['text'].values\n",
                "y_train = (train_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "X_val = val_df['text'].values\n",
                "y_val = (val_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "X_test = test_df['text'].values\n",
                "y_test = (test_df['label'] == 'spam').astype(int).values\n",
                "\n",
                "print(\"Data prepared successfully!\")\n",
                "print(f\"Training labels - Ham: {(y_train == 0).sum()}, Spam: {(y_train == 1).sum()}\")\n",
                "print(f\"Validation labels - Ham: {(y_val == 0).sum()}, Spam: {(y_val == 1).sum()}\")\n",
                "print(f\"Test labels - Ham: {(y_test == 0).sum()}, Spam: {(y_test == 1).sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helper-header",
            "metadata": {},
            "source": [
                "## 5. Helper Functions for Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
                "    \"\"\"Calculate comprehensive metrics including AUCPR\"\"\"\n",
                "    metrics = {\n",
                "        'accuracy': accuracy_score(y_true, y_pred),\n",
                "        'precision': precision_score(y_true, y_pred),\n",
                "        'recall': recall_score(y_true, y_pred),\n",
                "        'f1_score': f1_score(y_true, y_pred),\n",
                "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
                "        'aucpr': average_precision_score(y_true, y_pred_proba)  # Area Under PR Curve\n",
                "    }\n",
                "    return metrics\n",
                "\n",
                "def print_metrics(metrics, dataset_name=\"\"):\n",
                "    \"\"\"Pretty print metrics\"\"\"\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"{dataset_name} Metrics\")\n",
                "    print(f\"{'='*50}\")\n",
                "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
                "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
                "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
                "    print(f\"F1 Score:  {metrics['f1_score']:.4f}\")\n",
                "    print(f\"ROC AUC:   {metrics['roc_auc']:.4f}\")\n",
                "    print(f\"AUCPR:     {metrics['aucpr']:.4f}\")\n",
                "    print(f\"{'='*50}\")\n",
                "\n",
                "print(\"Helper functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model1-header",
            "metadata": {},
            "source": [
                "## 6. Model 1: Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model1-train",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Logistic_Regression\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Logistic Regression',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'C': 1.0,\n",
                "        'max_iter': 1000,\n",
                "        'solver': 'liblinear'\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', LogisticRegression(C=1.0, max_iter=1000, solver='liblinear', random_state=42))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Logistic Regression...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_LogisticRegression\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nâœ“ Model registered as 'SMS_Spam_LogisticRegression'\")\n",
                "    print(f\"âœ“ Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    lr_run_id = run.info.run_id\n",
                "    lr_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model2-header",
            "metadata": {},
            "source": [
                "## 7. Model 2: Multinomial Naive Bayes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model2-train",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Naive_Bayes\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Multinomial Naive Bayes',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'alpha': 1.0\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', MultinomialNB(alpha=1.0))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Multinomial Naive Bayes...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_NaiveBayes\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nâœ“ Model registered as 'SMS_Spam_NaiveBayes'\")\n",
                "    print(f\"âœ“ Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    nb_run_id = run.info.run_id\n",
                "    nb_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model3-header",
            "metadata": {},
            "source": [
                "## 8. Model 3: Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model3-train",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=\"Random_Forest\") as run:\n",
                "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
                "    \n",
                "    # Model parameters\n",
                "    params = {\n",
                "        'model_type': 'Random Forest',\n",
                "        'max_features': 5000,\n",
                "        'ngram_range': '(1, 2)',\n",
                "        'n_estimators': 100,\n",
                "        'max_depth': 20,\n",
                "        'min_samples_split': 5,\n",
                "        'min_samples_leaf': 2\n",
                "    }\n",
                "    \n",
                "    # Log parameters\n",
                "    mlflow.log_params(params)\n",
                "    \n",
                "    # Create pipeline\n",
                "    model = Pipeline([\n",
                "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
                "        ('classifier', RandomForestClassifier(\n",
                "            n_estimators=100,\n",
                "            max_depth=20,\n",
                "            min_samples_split=5,\n",
                "            min_samples_leaf=2,\n",
                "            random_state=42,\n",
                "            n_jobs=-1\n",
                "        ))\n",
                "    ])\n",
                "    \n",
                "    # Train model\n",
                "    print(\"\\nTraining Random Forest...\")\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
                "    \n",
                "    y_val_pred = model.predict(X_val)\n",
                "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
                "    \n",
                "    y_test_pred = model.predict(X_test)\n",
                "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
                "    val_metrics = calculate_metrics(y_val, y_val_pred, y_val_proba)\n",
                "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
                "    \n",
                "    # Log metrics to MLflow\n",
                "    for metric_name, value in train_metrics.items():\n",
                "        mlflow.log_metric(f\"train_{metric_name}\", value)\n",
                "    for metric_name, value in val_metrics.items():\n",
                "        mlflow.log_metric(f\"val_{metric_name}\", value)\n",
                "    for metric_name, value in test_metrics.items():\n",
                "        mlflow.log_metric(f\"test_{metric_name}\", value)\n",
                "    \n",
                "    # Print results\n",
                "    print_metrics(train_metrics, \"Training\")\n",
                "    print_metrics(val_metrics, \"Validation\")\n",
                "    print_metrics(test_metrics, \"Test\")\n",
                "    \n",
                "    # Log model to MLflow\n",
                "    mlflow.sklearn.log_model(\n",
                "        model,\n",
                "        \"model\",\n",
                "        registered_model_name=\"SMS_Spam_RandomForest\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nâœ“ Model registered as 'SMS_Spam_RandomForest'\")\n",
                "    print(f\"âœ“ Test AUCPR: {test_metrics['aucpr']:.4f}\")\n",
                "    \n",
                "    rf_run_id = run.info.run_id\n",
                "    rf_aucpr = test_metrics['aucpr']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison-header",
            "metadata": {},
            "source": [
                "## 9. Model Comparison - AUCPR Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Retrieve all runs from the experiment\n",
                "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
                "\n",
                "# Filter and display AUCPR metrics\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"MODEL COMPARISON - AUCPR (Area Under Precision-Recall Curve)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Sort by test AUCPR (descending)\n",
                "runs_sorted = runs.sort_values('metrics.test_aucpr', ascending=False)\n",
                "\n",
                "print(f\"\\n{'Model':<30} {'Test AUCPR':<15} {'Val AUCPR':<15} {'Train AUCPR':<15}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "for idx, row in runs_sorted.iterrows():\n",
                "    model_name = row['tags.mlflow.runName']\n",
                "    test_aucpr = row['metrics.test_aucpr']\n",
                "    val_aucpr = row['metrics.val_aucpr']\n",
                "    train_aucpr = row['metrics.train_aucpr']\n",
                "    \n",
                "    print(f\"{model_name:<30} {test_aucpr:<15.4f} {val_aucpr:<15.4f} {train_aucpr:<15.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "\n",
                "# Find best model\n",
                "best_model_row = runs_sorted.iloc[0]\n",
                "best_model_name = best_model_row['tags.mlflow.runName']\n",
                "best_aucpr = best_model_row['metrics.test_aucpr']\n",
                "\n",
                "print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
                "print(f\"   Test AUCPR: {best_aucpr:.4f}\")\n",
                "print(f\"   Run ID: {best_model_row['run_id']}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retrieve-header",
            "metadata": {},
            "source": [
                "## 10. Retrieve and Print Individual Model AUCPR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "retrieve-models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Retrieve AUCPR for each model from MLflow\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"INDIVIDUAL MODEL AUCPR RETRIEVAL FROM MLFLOW\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "model_names = [\n",
                "    \"Logistic_Regression\",\n",
                "    \"Naive_Bayes\",\n",
                "    \"Random_Forest\"\n",
                "]\n",
                "\n",
                "for model_name in model_names:\n",
                "    # Search for runs with this name\n",
                "    runs = mlflow.search_runs(\n",
                "        experiment_ids=[experiment.experiment_id],\n",
                "        filter_string=f\"tags.mlflow.runName = '{model_name}'\",\n",
                "        order_by=[\"start_time DESC\"],\n",
                "        max_results=1\n",
                "    )\n",
                "    \n",
                "    if len(runs) > 0:\n",
                "        run = runs.iloc[0]\n",
                "        run_id = run['run_id']\n",
                "        test_aucpr = run['metrics.test_aucpr']\n",
                "        val_aucpr = run['metrics.val_aucpr']\n",
                "        train_aucpr = run['metrics.train_aucpr']\n",
                "        \n",
                "        print(f\"\\nðŸ“Š {model_name}\")\n",
                "        print(f\"   Run ID: {run_id}\")\n",
                "        print(f\"   Test AUCPR:       {test_aucpr:.4f}\")\n",
                "        print(f\"   Validation AUCPR: {val_aucpr:.4f}\")\n",
                "        print(f\"   Training AUCPR:   {train_aucpr:.4f}\")\n",
                "        print(f\"   {'-'*50}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "registry-header",
            "metadata": {},
            "source": [
                "## 11. View Registered Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "registry-view",
            "metadata": {},
            "outputs": [],
            "source": [
                "from mlflow.tracking import MlflowClient\n",
                "\n",
                "client = MlflowClient()\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"REGISTERED MODELS IN MLFLOW MODEL REGISTRY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# List all registered models\n",
                "registered_models = client.search_registered_models()\n",
                "\n",
                "for rm in registered_models:\n",
                "    if rm.name.startswith(\"SMS_Spam_\"):\n",
                "        print(f\"\\nModel Name: {rm.name}\")\n",
                "        print(f\"Description: {rm.description if rm.description else 'N/A'}\")\n",
                "        print(f\"Latest Version: {rm.latest_versions[0].version if rm.latest_versions else 'N/A'}\")\n",
                "        print(f\"Last Updated: {rm.last_updated_timestamp}\")\n",
                "        print(\"-\"*70)\n",
                "\n",
                "print(\"\\nâœ“ All models successfully registered and tracked with MLflow!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}